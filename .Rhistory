tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(x)
tweetpt <- paste(tweetpt$text, collapse = ' ')
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt <- tm_map(tweetpt, PlainTextDocument)
tweetpt <- twListToDF(x)
tweetpt <- paste(tweetpt$text, collapse = ' ')
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(x)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(x)
tweetpt <- paste(tweetpt$text, collapse = ' ')
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("#Rio2016",n=1000, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("#Rio2016",n=2000, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
head(tweetpt)
tweetpt$text
stopwords('pt')
tweets <- searchTwitter("#Rio2016",n=100, lang = 'pt')
tweetpt <- twListToDF(tweets)
#clean data
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
s_words = c("sobre","após","httpstcopeidluqlc","seis")
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
#load tweets
tweets <- searchTwitter("#Rio2016",n=100, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
s_words <- c("sobre","após","httpstcopeidluqlc","seis")
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
#load tweets
tweets <- searchTwitter("#Rio2016",n=500, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
s_words <- c("sobre","após","httpstcopeidluqlc","seis")
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
#load tweets
tweets <- searchTwitter("#Rio2016",n=5000, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("#Rio2016",n=500, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("#Rio2016",n=1000, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
set.seed(999)
wordcloud(tweetpt, min.freq = 1,max.words=150, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
set.seed(11)
wordcloud(tweetpt, min.freq = 1,max.words=150, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
set.seed(11)
wordcloud(tweetpt, min.freq = 1,max.words=150, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("#Rio2016",n=700, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
head(tweetpt)
#wordcloud
set.seed(11)
wordcloud(tweetpt, min.freq = 1,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
set.seed(11)
wordcloud(tweetpt, min.freq = 3,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
data.frame(tweetpt)
document_tm <- TermDocumentMatrix(tweetpt)
document_tm_mat <- as.matrix(document_tm)
View(document_tm_mat)
set.seed(11)
wordcloud(tweetpt, min.freq = 15,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud(tweetpt, min.freq = 5,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = gsub("xedxaxbdxedxbxa+","",tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
set.seed(11)
wordcloud(tweetpt, min.freq = 4,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt$text
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweets$text, 'UTF-8', 'latin1', 'byte')
tweetpt
tweets$text
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt
tweetpt <- gsub("<*>","",tweetpt)
tweetpt = gsub("<*>","",tweetpt)
tweetpt
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt
tweetpt = gsub("^<.+>$","",tweetpt)
tweetpt
tweetpt = gsub("<.+>","",tweetpt)
tweetpt
#load tweets
tweets <- searchTwitter("#Rio2016",n=700, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = gsub("<.+>","",tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(11)
wordcloud(tweetpt, min.freq = 4,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt <- paste(tweetpt$text, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(11)
wordcloud(tweetpt, min.freq = 4,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt <- paste(tweetpt, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
#wordcloud
set.seed(11)
wordcloud(tweetpt, min.freq = 4,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
setwd("/media/guilherme/1e8ee6ef-1ae9-40f8-920c-939d24a7a626/guilherme/DataScience/BigDataAnalytics_Azure/Projeto_1/")
set.seed(11)
png("Rio2016_WordCloud.png", width = 480, height = 480)
wordcloud(tweetpt, min.freq = 4,max.words=170, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
dev.off()
set.seed(11)
png("Rio2016_WordCloud.png", width = 480, height = 480)
wordcloud(tweetpt, min.freq = 4,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
dev.off()
set.seed(11)
png("Rio2016_WordCloud.png", width = 380, height = 380)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
dev.off()
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=1.4)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=5)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.95, colors=brewer.pal(8, "Dark2"),title.size=5)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10) %+% title("aaaaa")
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10) %>% title("aaaaa")
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10) %>% title("Olimpiadas Rio2016")
?title
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10) %>% title("Olimpiadas Rio2016",font=10)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10) %>% title("Olimpiadas Rio2016",font=20)
wordcloud(tweetpt,main="Rio" , min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"),title.size=10)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweettdm <- TermDocumentMatrix(tweetpt)
tweettdm
findFreqTerms(tweettdm, lowfreq = 11)
arecem com mais frequência
findFreqTerms(tweettdm, highfreq = 11)
findFreqTerms(tweettdm, highfreq = 3)
findFreqTerms(tweettdm, lowfreq = 11)
findFreqTerms(tweettdm, lowfreq = 3)
findFreqTerms(tweettdm, lowfreq = 55)
findFreqTerms(tweettdm, lowfreq = 10)
tweetpt = gsub("https$|https[A-z]+$","",tweetpt)
tweets <- searchTwitter("#Rio2016",n=700, lang = 'pt')
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt = gsub("https$|https[A-z]+$","",tweetpt)
tweetpt <- paste(tweetpt, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweettdm <- TermDocumentMatrix(tweetpt)
# Encontrando as palavras que aparecem com mais frequência
findFreqTerms(tweettdm, lowfreq = 10)
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt <- paste(tweetpt, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = gsub("https$|https[A-z]+$","",tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweettdm <- TermDocumentMatrix(tweetpt)
# Encontrando as palavras que aparecem com mais frequência
findFreqTerms(tweettdm, lowfreq = 10)
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("<.+>","",tweetpt)
tweetpt
tweetpt = gsub("https$|https://[A-z]+$","",tweetpt)
tweetpt
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("https:\/\/.+","",tweetpt)
tweetpt = gsub("https://.+","",tweetpt)
tweetpt = gsub("<.+>","",tweetpt)
tweetpt
tweetpt <- twListToDF(tweets)
tweetpt <- iconv(tweetpt$text, 'UTF-8', 'latin1', 'byte')
tweetpt = gsub("https://.+","",tweetpt)
tweetpt = gsub("<.+>","",tweetpt)
tweetpt <- paste(tweetpt, collapse = ' ')
#clean data
s_words <- c("sobre","após","httpstcopeidluqlc","seis","xedxaxbdxedxbxa","well","superb")
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
tweetpt = Corpus(VectorSource(tweetpt))
tweetpt <- tm_map(tweetpt, content_transformer(tolower))
tweetpt <- tm_map(tweetpt, removeNumbers)
tweetpt <- tm_map(tweetpt, removePunctuation)
tweetpt = tm_map(tweetpt, removeWords, stopwords('pt'))
tweetpt = tm_map(tweetpt, removeWords, s_words)
tweetpt <- tm_map(tweetpt, PlainTextDocument)
wordcloud(tweetpt, min.freq = 3,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
tweettdm <- TermDocumentMatrix(tweetpt)
# Encontrando as palavras que aparecem com mais frequência
findFreqTerms(tweettdm, lowfreq = 10)
findAssocs(tweettdm, 'abandono', 0.60)
tweet2tdm <-removeSparseTerms(tweettdm, sparse = 0.9)
tweet2tdmscale <- scale(tweet2tdm)
# Distance Matrix
tweetdist <- dist(tweet2tdmscale, method = "euclidean")
# Preprando o dendograma
tweetfit <- hclust(tweetdist)
# Criando o dendograma (verificando como as palvras se agrupam)
plot(tweetfit)
m com mais frequência
findFreqTerms(tweettdm, lowfreq = 30)
findAssocs(tweettdm, 'abandono', 0.60)
tweet2tdm <-removeSparseTerms(tweettdm, sparse = 0.9)
tweet2tdmscale <- scale(tweet2tdm)
tweetdist <- dist(tweet2tdmscale, method = "euclidean")
tweetfit <- hclust(tweetdist)
plot(tweetfit)
tweetdist
tweetdist[10][10]
tweetdist[1:10,1:10]
tweetdist[1:10][1:10]
head(tweetdist)
# Removendo termos esparsos (não utilizados frequentemente)
tweet2tdm
tweet2tdm
tweet2tdmscale
tweetdist
ontrando as palavras que aparecem com mais frequência
t <- findFreqTerms(tweettdm, lowfreq = 30)
ssociações
findAssocs(t, 'abandono', 0.60)
tweettdm
t
t2 <- TermDocumentMatrix(t)
t = Corpus(VectorSource(t))
t2 <- TermDocumentMatrix(t)
t2
findAssocs(t, 'abandono', 0.60)
findAssocs(t2, 'abandono', 0.60)
quentemente)
tweet2tdm <-removeSparseTerms(t2, sparse = 0.9)
tweet2tdmscale <- scale(tweet2tdm)
tweetdist <- dist(tweet2tdmscale, method = "euclidean")
tweetfit <- hclust(tweetdist)
